# 决策树



## 基础知识

决策树的逻辑就像是在玩“20个问题”游戏，或者像医生看病：通过不断提出问题（特征判断），将复杂的局面一步步细分，直到得出最终结论。

以下是决策树的核心构建逻辑：

**核心结构**

- **根节点 (Root Node)**：包含所有样本的集合，是决策的起点。
- **内部节点 (Internal Nodes)**：中间的判断点，比如“天气是晴天吗？”或者“拥有房产吗？”。
- **叶节点 (Leaf Nodes)**：最终的决策结果（类别），比如“借贷给这个人”或“拒绝借贷”。

**核心难题：该问哪个问题？**

构建一棵树最关键的一步是：**在当前节点，我应该选择哪一个特征（Attribute）来进行分裂？**

比如你有“年龄”、“收入”、“学历”三个特征。你是先按“年龄”分，还是先按“收入”分？

- **原则**：我们希望每一次分裂，都能让剩下的数据变得更加**“纯净” (Pure)**。
- **目标**：如果分完之后，左边全是“好人”，右边全是“坏人”，那就是完美的分裂。如果两边还是好坏参半，那就是糟糕的分裂。

为了衡量这种“纯净度”，我们引入了两个数学指标：**信息熵（Entropy）** 和 **基尼系数（Gini Impurity）**。

---

**信息熵 (Entropy) **

这是物理学概念在信息论中的延伸。熵越大，表示系统越混乱（纯度越低）；熵越小，表示越有序（纯度越高）。

公式：假设样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$，则熵定义为：

$$Ent(D) = - \sum_{k=1}^{n} p_k \log_2 p_k$$

**直观理解**：

- 如果集合里全是“好人” ($p_1=1, p_2=0$)，则 $Ent(D) = 0$（最纯净）。
- 如果好人坏人各一半 ($p_1=0.5, p_2=0.5$)，则 $Ent(D) = 1$（最混乱）。

信息增益 (Information Gain)：父节点的熵减去所有子节点熵的加权和。

$$\text{Gain}(D, a) = Ent(D) - \sum \frac{|D^v|}{|D|} Ent(D^v)$$

**决策逻辑**：ID3 算法就是计算所有特征的信息增益，选择**增益最大**（即混乱度下降最快）的那个特征来分裂。

---

**基尼系数 (Gini Impurity) **

这是经济学概念的借用。它衡量的是：从集合中随机抽取两个样本，它们类别不一致的概率。

公式：

$$\text{Gini}(D) = 1 - \sum_{k=1}^{n} p_k^2$$

**直观理解**：

- 如果全是同一类，$\sum p^2 = 1$，则 $\text{Gini} = 0$（最纯净）。
- 如果两类各一半，$\sum p^2 = 0.5^2 + 0.5^2 = 0.5$，则 $\text{Gini} = 0.5$（最不纯）。

**决策逻辑**：CART 算法（Classification and Regression Tree）使用基尼系数。它会选择让分裂后的基尼系数**最小化**的特征。

---

**著名的算法家族**

- **ID3**：最古老的算法。只支持离散特征（比如“天气=晴/阴/雨”），使用**信息增益**。缺点是它偏爱取值多的特征（比如按“身份证号”分，每个人都不一样，纯度最高，但这没意义）。
- **C4.5**：ID3 的改进版。
  - 使用**信息增益率 (Gain Ratio)**：除以一个惩罚项，解决了偏爱多值特征的问题。
  - 能处理连续特征（比如“年龄>30”）。
  - 能处理缺失值。
- **CART**：目前最流行的算法（sklearn 默认使用）。
  - 使用**基尼系数**。
  - 生成的必然是**二叉树**（每次只分两叉，Yes or No）。
  - 既能做分类，也能做回归（回归树用平方误差最小化）。

---

**小结**

- **目的**：把乱糟糟的数据切成整齐的小块。
- **手段**：贪心算法。每一步都只看眼前，选那个能让数据变纯得最快的特征切一刀。
- **标尺**：熵（ID3/C4.5）或者基尼系数（CART）。

这就是决策树的生长原理。但是，决策树有一个致命的弱点——**非常容易过拟合（Overfitting）**。如果树长得太深，它会把训练集里的噪声也当作规律记下来。

------

### ID3 算法

假设我们有 14 天的历史数据，记录了天气情况和最后是否去打网球。我们的目标是构建一棵决策树来预测未来。

**数据集预览（共 14 个样本）：**

- **Target (标签)**：Play Tennis (9个 Yes, 5个 No)
- **Feature (特征)**：
  1. Outlook (天气)：Sunny (5), Overcast (4), Rain (5)
  2. Humidity (湿度)：High (7), Normal (7)
  3. Wind (风)：Weak (8), Strong (6)
  4. Temperature (温度)：...（省略，仅以此为例）

**Step 1: 计算根节点的全局信息熵 (Global Entropy)**

首先，我们要看看在没有任何特征划分的情况下，数据有多乱。

样本总数 $D=14$，其中 Yes=$9$，No=$5$。

$$H(D) = - \frac{9}{14} \log_2(\frac{9}{14}) - \frac{5}{14} \log_2(\frac{5}{14}) \approx 0.940$$

**Step 2: 尝试用不同特征进行分裂，计算信息增益**

我们要分别计算按“天气”、“湿度”、“风”分裂后的收益，看谁最大。

**场景 A：按照 Outlook (天气) 分裂**

我们将 14 个样本按天气分成三堆：

1. D_Sunny (5天)：2 Yes, 3 No。

   

   $$H(\text{Sunny}) = - \frac{2}{5} \log_2 \frac{2}{5} - \frac{3}{5} \log_2 \frac{3}{5} \approx 0.971$$

   

2. D_Overcast (4天)：4 Yes, 0 No。

   

   $$H(\text{Overcast}) = 0$$

    (纯度最高，全是 Yes！)

   

3. D_Rain (5天)：3 Yes, 2 No。

   

   $$H(\text{Rain}) = - \frac{3}{5} \log_2 \frac{3}{5} - \frac{2}{5} \log_2 \frac{2}{5} \approx 0.971$$

   

计算 Outlook 的信息增益 (Gain)：

$$\text{Gain}(D, \text{Outlook}) = H(D) - \sum \frac{|D_v|}{|D|} H(D_v)$$

$$\text{Gain} = 0.940 - [\frac{5}{14}(0.971) + \frac{4}{14}(0) + \frac{5}{14}(0.971)]$$

$$\text{Gain} = 0.940 - 0.693 = \mathbf{0.247}$$

**场景 B：按照 Humidity (湿度) 分裂**

同理计算（省略中间过程）：

- High 组：3 Yes, 4 No -> 熵较高
- Normal 组：6 Yes, 1 No -> 熵较低
- 最终算出：$\text{Gain}(D, \text{Humidity}) \approx \mathbf{0.151}$

**场景 C：按照 Wind (风) 分裂**

- 最终算出：$\text{Gain}(D, \text{Wind}) \approx \mathbf{0.048}$

**Step 3: 选择最佳特征并递归**

- **比较**：Outlook (0.247) > Humidity (0.151) > Wind (0.048)。
- **决策**：根节点选择 **Outlook**。

树生长出三个分支：

1. **Overcast 分支**：对应的子集全是 Yes，**熵为 0**。停止分裂，生成一个叶节点：**"Yes"**。
2. **Sunny 分支**：剩下的 5 天数据（2 Yes, 3 No）还不纯。我们需要在“湿度”和“风”中继续挑选特征，重复上述 Step 2 的过程。
3. **Rain 分支**：剩下的 5 天数据（3 Yes, 2 No）也不纯。继续递归。

最终，你会得到一棵完整的树。

------

### 剪枝 (Pruning) 

决策树有一个天然的倾向：死记硬背。如果不加限制，它会不停地分裂，直到每个叶子节点里只有一个样本（熵为0）。

这会导致树变得极其庞大且复杂，虽然在训练集上准确率 100%，但在新数据上表现极差（过拟合）。

为了解决这个问题，我们需要**剪枝**。主要有两种策略：

**1. 预剪枝 (Pre-Pruning) —— 见好就收**

在树构建的过程中，提前停止分裂。这是一些“硬性规定”。

- **限制深度 (Max Depth)**：规定树最高只能长 3 层，到了 3 层无论纯不纯都强制停止。
- **限制节点样本数 (Min Samples Split)**：如果当前节点的样本少于 10 个，就不再分了，直接由多数投票决定类别。
- **限制增益阈值 (Min Impurity Decrease)**：如果这一次分裂带来的信息增益小于 0.01（提升微乎其微），那就不分了，省得麻烦。
- **优点**：训练速度快，模型简单。
- **缺点**：容易**欠拟合 (Underfitting)**。因为有时候现在的分裂虽然收益小，但也许下一次分裂会有巨大的收益（视界效应），预剪枝会错失良机。

**2. 后剪枝 (Post-Pruning) —— 秋后算账**

先让树自由生长，长成一棵茂盛的完全树，然后从底部向上评估：**“如果我剪掉这个树枝，把它变成一个叶子，模型的泛化性能会不会变好？”**

最著名的后剪枝算法是 **代价复杂度剪枝 (Cost-Complexity Pruning, CCP)**。

我们需要最小化以下损失函数：

$$C_{\alpha}(T) = \text{Error}(T) + \alpha \cdot |T|$$

- **$\text{Error}(T)$**：模型在验证集上的误差（或者训练集的拟合误差）。
- **$|T|$**：树的叶子节点个数（代表复杂度）。
- **$\alpha$**：惩罚系数（类似正则化里的 $\lambda$）。

**逻辑如下：**

1. 如果剪掉某棵子树，虽然训练误差 $\text{Error}(T)$ 可能会稍微增加一点点。
2. 但是，树的复杂度 $|T|$ 会大幅下降。
3. 如果 $\text{Error}$ 增加的代价 < 复杂度下降带来的收益，那么**剪！**

- **优点**：保留了重要的分支，通常泛化能力比预剪枝强，不易欠拟合。
- **缺点**：计算开销大，因为要先生成完整的树，再回过头来计算。

**总结**：

- **ID3** 是一步步贪心地找信息增益最大的特征。
- **剪枝** 是为了砍掉那些为了迎合噪声而长出来的细枝末节，回归事物的本质规律。

---

### CART

**CART 算法 (Classification and Regression Tree)** 是目前应用最广泛的决策树算法（Python 的 `scikit-learn` 库默认使用的决策树实现就是 CART）。

正如它的名字所示，它不仅能做**分类**（区分好人坏人），还能做**回归**（预测房价、温度等具体数值）。

它与 ID3/C4.5 最大的区别在于：**CART 构建的是“二叉树” (Binary Tree)**。它非常固执，每次分裂只问“Yes or No”，哪怕特征有多个取值，它也强行切成两半。

以下是 CART 的核心机制详解：

------

**结构特征：严格的二叉树 (Binary Splitting)**

- **ID3/C4.5 的做法**：如果“天气”有 {晴, 阴, 雨} 三种取值，ID3 会直接分出 3 个叉。
- **CART 的做法**：它只会切两刀。
  - 它会尝试：“是晴天吗？” -> (Yes: 晴天) vs (No: 阴天/雨天)。
  - 或者尝试：“是雨天吗？” -> (Yes: 雨天) vs (No: 晴天/阴天)。
  - 它会计算哪种切分方式的纯度更高，就选哪种。

为什么这么做？

二叉树结构更加简洁，且避免了 ID3 那种“特征取值越多越容易被选中”的缺陷（虽然 C4.5 用增益率解决了这个问题，但二叉树在工程实现上更高效）。

------

**分类树 (Classification Tree)** 

当 CART 用于分类任务时，它使用 **基尼系数 (Gini Impurity)** 来选择最佳分裂点。

公式回顾：



$$Gini(D) = 1 - \sum_{k=1}^{K} p_k^2$$



其中 $p_k$ 是样本属于第 $k$ 类的概率。

选择标准：

我们要找一个特征 $A$ 和一个切分点 $t$，把集合 $D$ 切分成 $D_1$（左子节点）和 $D_2$（右子节点），使得加权后的基尼系数最小：



$$Gini_{split} = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)$$



- **特点**：Gini 系数只需要做平方运算，比信息熵的对数运算 ($\log$) 快一点点。在实际应用中，两者的效果几乎没有区别。

------

**回归树 (Regression Tree)** 

这是 CART 最强大的地方。决策树怎么预测一个连续的数值（比如明天的气温是 25.4 度）？

**预测策略：**

- **叶节点的值**：落在这个叶子节点里所有训练样本的**平均值 (Mean)**。
  - 比如一个叶子节点里有 3 个样本，房价分别是 [100万, 110万, 120万]，那么这个节点的预测输出就是 110万。

分裂标准：最小化平方误差 (Least Squared Deviation, LSD)

我们要找一个切分点，使得切分后，两个子节点里的样本方差最小（也就是大家都很接近平均值，没有离群点）。

目标是最小化以下损失函数：

$$\min_{j, s} \left[ \sum_{x_i \in R_1(j,s)} (y_i - \bar{y}_1)^2 + \sum_{x_i \in R_2(j,s)} (y_i - \bar{y}_2)^2 \right]$$

这里每个符号的严格含义如下：

- **$x_i$**：表示训练集中的第 $i$ 个**样本向量**（例如：样本 $i$ 的 [身高, 体重, 年龄]）。
- **$j$**：表示**特征的索引**（Splitting Variable）。
  - 我们在遍历每一个特征。比如 $j=1$ 代表“身高”，$j=2$ 代表“体重”。
- **$s$**：表示**切分点的值**（Splitting Point）。
  - 我们在遍历该特征下的所有可能取值。比如身高特征下，s 可以是 170cm, 175cm 等。
- **$R_1(j, s)$ 和 $R_2(j, s)$**：
  - 这不是固定的区域，而是**由 $j$ 和 $s$ 决定的区域**。
  - $R_1(j, s) = \{ x | x^{(j)} \le s \}$：所有在特征 $j$ 上数值小于等于 $s$ 的样本集合。
  - $R_2(j, s) = \{ x | x^{(j)} > s \}$：所有在特征 $j$ 上数值大于 $s$ 的样本集合。
- **$\bar{y}_1$ 和 $\bar{y}_2$**：
  - $\bar{y}_1$ 是 $R_1$ 区域内所有样本标签 $y$ 的**平均值**（即：预测值）。
  - $\bar{y}_2$ 是 $R_2$ 区域内所有样本标签 $y$ 的**平均值**。

------

**这个公式在做什么？**

这个 $\min_{j, s}$ 实际上描述了一个**两层嵌套的搜索过程**（贪心算法），目的是找到切分数据的“完美一刀”。

**外层循环：遍历每一个特征 $j$**

- 假设我们有“年龄”和“收入”两个特征。
- 先试 $j=$ 年龄，再试 $j=$ 收入。

> [!note]
>
> CART 处理采用的是**二分法**。
>
> 假设“身高”特征有 3 个值：$\{160, 170, 180\}$。
>
> 1. **排序**：先把特征值从小到大排好。
> 2. **找切分点**：取相邻两个数的中间值作为候选切分点。
>    - 切分点 1：$165$ ($160$ 和 $170$ 的中间)。判断：身高 $\le 165$？
>    - 切分点 2：$175$ ($170$ 和 $180$ 的中间)。判断：身高 $\le 175$？
> 3. **遍历计算**：计算每一个切分点的 Gini 系数或平方误差，选最好的那个。

**内层循环：遍历每一个切分点 $s$**

- 当固定 $j=$ 年龄时，把所有样本按年龄排序。
- 尝试把刀切在 20 岁 ($s=20$)：
  - 左边 ($R_1$)：20 岁以下的样本。算算它们的方差（平方误差和）。
  - 右边 ($R_2$)：20 岁以上的样本。算算它们的方差（平方误差和）。
  - **总损失 = 左边误差 + 右边误差**。
- 尝试把刀切在 30 岁 ($s=30$)：
  - 重新计算左边和右边的总误差。
- ...遍历所有可能的年龄切分点。

**最终目标 ($\min$)**

- 找到那一对组合 **$(j^*, s^*)$**，使得这一刀切下去后，左右两堆数据的**内部纯度最高（误差最小）**。

几何意义：

回归树本质上是用阶梯状的矩形去拟合一条曲线。树越深，阶梯越密，拟合得越精细（但也越容易过拟合）。

> [!example]
>
> 假设我们要预测一个人的体重 ($y$)，特征只有一个：身高 ($x$)。
>
> 样本数据：
>
> - A: 150cm, 45kg
> - B: 160cm, 50kg
> - C: 180cm, 80kg
> - D: 190cm, 90kg
>
> 我们尝试寻找切分点 $s$：
>
> **尝试切分点 $s = 155$ (在 A 和 B 之间切)**
>
> - **$R_1$ (矮个组)**：只有 A。$\bar{y}_1 = 45$。误差 $\sum (y - 45)^2 = 0$。
> - **$R_2$ (高个组)**：B, C, D。平均值 $\bar{y}_2 = (50+80+90)/3 \approx 73.3$。
> - **$R_2$ 的误差**：$(50-73.3)^2 + (80-73.3)^2 + (90-73.3)^2$ (误差很大，因为 50 和 90 差很远)。
> - **总损失**：很大。
>
> **尝试切分点 $s = 170$ (在 B 和 C 之间切)**
>
> - **$R_1$ (矮个组)**：A, B。平均值 $\bar{y}_1 = 47.5$。数据都在 45-50 之间，误差很小。
> - **$R_2$ (高个组)**：C, D。平均值 $\bar{y}_2 = 85$。数据都在 80-90 之间，误差也很小。
> - **总损失**：最小！
>
> **结论**：CART 算法会选择 **$j=$身高, $s=170$** 作为最佳分裂点。

------

**剪枝策略 (CCP)**

CART 的作者（Breiman 等人）提出了著名的 **代价复杂度剪枝 (Cost-Complexity Pruning, CCP)**。

$$C_{\alpha}(T) = R(T) + \alpha |T|$$

- $R(T)$：训练误差（拟合程度）。
- $|T|$：叶子节点数量（模型复杂度）。
- $\alpha$：惩罚力度。

CART 算法不仅生成树，还会自动通过交叉验证计算出一系列最优的 $\alpha$ 值，帮你找到那颗“刚刚好”的树。