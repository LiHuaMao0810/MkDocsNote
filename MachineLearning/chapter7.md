# KNN & Metric Learning

## KNN

KNN (K-Nearest Neighbors) 的逻辑非常直观：

假设你不知道这杯神秘液体是什么，但你发现它依然成分和“可乐”最接近，口感和“可乐”最接近，颜色和“可乐”最接近。根据 KNN 原则，它大概率就是可乐。

**算法步骤：**

1. 计算测试点 $x$ 与训练集中**所有** $N$ 个点的距离。
2. 排序，找出距离最近的 $K$ 个点。
3. **分类**：这 $K$ 个点里谁多（投票），$x$ 就属于谁。
4. **回归**：求这 $K$ 个点的平均值。

致命缺点：慢！

这就是所谓的 Exact KNN（精确搜索）。

想象一下，如果你有 10 亿 (1B) 条数据（比如淘宝的商品库），来了一个新用户，你要给他推荐商品。

你需要计算 10 亿次距离，然后排序。等算完了，用户早就关掉 APP 睡觉了。

它的复杂度是 $O(N \cdot D)$（N是数据量，D是维度）。在大数据时代，这是不可接受的。

------

## AKNN Search

**AKNN (Approximate KNN) —— 速度与精度的权衡**

为了解决“慢”的问题，**AKNN (近似 K 近邻搜索)** 诞生了。这也是现在 **向量数据库 (Vector Database)** 和 **RAG (检索增强生成)** 的核心技术。

核心思想：

我不追求 100% 找到“最近”的那几个点，我只要找到“足够近”的点，但速度要快 100 倍甚至 1000 倍。

- **Exact KNN**：遍历所有数据，耗时 1秒，准确率 100%。
- **AKNN**：只看一部分数据，耗时 1毫秒，准确率 98%。

AKNN 有三大主流流派：

**1 基于树 (Trees) —— 像图书馆分类**

比如 **KD-Tree** 或 **Annoy**（Spotify 开源的）。

- **原理**：通过不断地用超平面把空间切成两半，把数据像挂在树叶上一样分层存储。
- **搜索**：顺着树枝往下找，只在相关的叶子节点里算距离，不需要遍历所有数据。
- **缺点**：在高维数据（维度 > 100）下，效果会退化成暴力搜索（维数灾难）。

**2 基于哈希 (LSH, Locality Sensitive Hashing) —— 像分桶**

- **原理**：设计一种神奇的哈希函数，保证**“原本相似的数据，哈希后的桶 (Bucket) 大概率相同”**。
- **搜索**：算出查询点的哈希值，直接去对应的桶里捞数据。
- **优点**：速度极快。

**3 基于图 (Graphs) —— HNSW (目前的王者)**

这是目前最强、最流行的算法（Milvus, FAISS, Weaviate 都在用）。全称 **Hierarchical Navigable Small World**。

原理 (高速公路隐喻)：

它构建了一个分层的图结构。

- **顶层 (高速公路)**：点很少，点之间距离很远。你可以快速跨越整个数据空间。
- **中层 (城市主干道)**：点变多了，范围缩小。
- **底层 (社区小路)**：包含所有数据，通过精细导航找到目标。

**搜索过程**：先在顶层“跳跃”找到大概位置，然后逐层下钻，最后在底层锁定目标。

**4 基于量化 (Quantization) —— IVF & PQ**

这是 Facebook 的 **FAISS** 库常用的技术。

- **IVF (倒排文件)**：先用 K-Means 把数据聚成 1000 个簇。查询时，先看离哪个簇中心最近，只去那个簇里找。
- **PQ (乘积量化)**：把高维向量压缩（比如把 float32 压缩成 int8），计算距离时直接用查表法，极大降低内存和计算开销。

------

**总结**

1. **KNN**：理论简单，但数据量大时**算不动**。
2. **AKNN**：现代 AI 的基石。它通过**建立索引**（图、树、哈希），牺牲一点点精度，换取了极致的搜索速度。

你现在用 ChatGPT 的知识库检索，或者淘宝的“以图搜图”，底层跑的全部都是 **AKNN** 算法。

---



## Metric Learning

**度量学习 (Metric Learning)**，简单来说，就是教会机器学习**“什么是相似的，什么是不同的”**。

它的核心不再是预测一个具体的标签（如“猫”或“狗”），而是学习一个**距离函数 (Distance Function)**。在这个新的“度量空间”里，**同类样本的距离尽可能小，不同类样本的距离尽可能大**。

---

**为什么我们需要 Metric Learning？（Open-set 问题）**

在之前的 SVM 或决策树中，我们处理的通常是**闭集 (Closed-set)** 问题：

- **训练集**：有 A、B、C 三类。
- **测试集**：还是分 A、B、C 三类。
- **方法**：用 Softmax 或超平面直接切分。

但是，现实世界往往是**开集 (Open-set)** 问题。

- **例子**：**人脸识别 (Face ID)**。
- **训练时**：模型看过张三、李四、王五的脸。
- **测试时**：突然来了个“赵六”（模型从未见过的人）。
- **问题**：Softmax 分类器会傻眼，因为它只有张三李四王五的类别节点。

Metric Learning 的解法：

我不关心你是谁，我只算出你这张照片和系统里存的照片之间的距离（相似度）。

- 如果距离 < 阈值，就认为是同一个人。
- 如果距离 > 阈值，就认为是陌生人。

------

**核心机制：Embedding Space (嵌入空间)**

Metric Learning 通常借助深度神经网络（如 CNN）来实现：

1. **输入**：原始数据（如图片）。
2. **映射**：通过神经网络提取特征，映射到一个低维向量空间（比如 128 维或 512 维）。这个向量叫 **Embedding**。
3. **度量**：在这个空间里计算欧氏距离或余弦相似度。

**目标**：通过训练，把同一类的点“捏”在一起，把不同类的点“推”开。

------

**核心驱动力：损失函数 (Loss Functions)**

这是 Metric Learning 的灵魂。由于没有固定的分类标签，我们需要设计特殊的 Loss 来告诉模型怎么“推拉”数据。

**A. 对比损失 (Contrastive Loss)**

这是最基础的形式。每次输入**一对 (Pair)** 样本 $(x_1, x_2)$。

- 如果它们是同类 ($y=1$)：我们要最小化它们的距离 $d(x_1, x_2)$。
- 如果它们是异类 ($y=0$)：我们要最大化它们的距离，直到超过一个**阈值 (Margin, $m$)** 为止。

$$L = y \cdot d^2 + (1-y) \cdot \max(0, m - d)^2$$

- **直观理解**：朋友要紧紧抱在一起；敌人如果离得太近（小于 $m$）就踹开，离得远了就不管了。

**B. 三元组损失 (Triplet Loss) —— 最经典**

这是 FaceNet 提出的方法，也是 Metric Learning 的成名作。

每次输入三个 (Triplet) 样本：

- **Anchor ($A$)**：锚点（基准样本）。
- **Positive ($P$)**：正样本（和 $A$ 同类）。
- **Negative ($N$)**：负样本（和 $A$ 异类）。

目标：我们要让 $A$ 和 $P$ 的距离，比 $A$ 和 $N$ 的距离，至少小一个 Margin ($\alpha$)。

$$d(A, P) + \alpha < d(A, N)$$

损失函数：

$$L = \max(0, d(A, P) - d(A, N) + \alpha)$$

- **Hard Negative Mining (难样本挖掘)**：Triplet Loss 训练时最关键的技巧。如果随机选 $N$，大部分 $N$ 离 $A$ 已经很远了，Loss 是 0，模型学不到东西。所以我们要专门挑那些**“长得像 $A$ 的异类”**（Hard Negatives）来训练，强迫模型学会区分细节。

------

### 4. 进阶：分类损失的变体 (Softmax-based)

Triplet Loss 虽然好，但训练数据组合爆炸（三元组的数量是立方的），很难收敛。

最近几年（如 ArcFace, CosFace），大家发现可以直接改造分类任务的 Softmax Loss。

- **原理**：在将特征映射到球面上时，强制要求同类样本在角度上更加紧凑（引入角度 Margin）。
- **优点**：像分类一样训练，收敛快，但学出来的特征具有极好的度量特性。

----

**应用场景**

1. **人脸识别**：FaceID、门禁系统。
2. **行人重识别 (Re-ID)**：在不同摄像头下追踪同一个人（穿着衣服、背影识别）。
3. **图像检索**：淘宝的“拍立淘”，上传一张衣服照片，找出相似的商品。
4. **少样本学习 (Few-shot Learning)**：只看一张照片，就要学会识别这个新物种（原型网络 Prototypical Networks）。

------

### 总结

**Metric Learning** 就是一种**“几何学”**思维：

- **分类器** 是在画**边界**（切分空间）。
- **度量学习** 是在做**映射**（扭曲空间）。它试图将高维的语义信息，压缩成低维空间里的几何距离。

这一块内容非常有趣，它是连接深度学习与几何直觉的桥梁。理解了它，你就理解了现在的 AI 是如何进行“人脸比对”或“指纹匹配”的。



## Mahalanobis Distance

**马氏距离 (Mahalanobis Distance)** 是聚类和度量学习中一个非常高阶且关键的概念。

简单一句话总结：**它是“修正了数据分布形状”后的欧氏距离。**

如果说欧氏距离是用一把刚性的尺子去测量绝对距离，那么马氏距离就是一把**“会根据数据的密度和相关性自动伸缩”**的尺子。

------

### 1. 为什么要提出马氏距离？（欧氏距离的缺陷）

在 K-Means 聚类中，我们使用欧氏距离，这隐含了两个假设：

1. **各特征独立**：假设特征之间没有相关性（比如身高和体重是无关的）。
2. **各特征方差一致**：假设数据的分布是圆球状的（Isotropic）。

但在现实中，数据往往是**椭球状**分布的，且特征之间有强相关性。

- **例子**：假设我们要聚类“身高”和“体重”。
  - 身高变化范围：150cm - 190cm（差值 40）。
  - 体重变化范围：40kg - 100kg（差值 60）。
  - 且身高越高，体重通常越重（正相关）。

如果你直接用欧氏距离，数据分布会是一个斜着的椭圆。欧氏距离会错误地认为：垂直于椭圆长轴方向的两个点，比顺着长轴方向的两个点更“近”，这违背了数据的统计规律。

------

### 2. 马氏距离的核心公式

马氏距离引入了**协方差矩阵 (Covariance Matrix, $\Sigma$)** 来修正距离。

两个样本点 $x$ 和 $y$ 之间的马氏距离 $D_M(x, y)$ 定义为：

$$D_M(x, y) = \sqrt{ (x - y)^T \Sigma^{-1} (x - y) }$$

- **$(x-y)$**：向量差（原始距离向量）。
- **$\Sigma^{-1}$**：协方差矩阵的**逆矩阵**。这是灵魂所在。

------

### 3. 马氏距离到底做了什么？（直观理解）

公式里的 $\Sigma^{-1}$ 其实做了两件事：

#### A. 消除量纲（Scale Invariance）

如果特征 A 的方差很大（分布很广），$\Sigma^{-1}$ 里对应的值就会很小。

- **效果**：把原本扁长的分布“压缩”，把原本窄窄的分布“拉伸”。这让所有特征的方差都归一化为 1。

#### B. 消除相关性（Decorrelation）

如果特征 A 和 B 高度相关（斜着的椭圆），$\Sigma^{-1}$ 会包含非对角线元素。

- **效果**：通过旋转坐标轴，消除特征间的相关性。

终极效果：

马氏距离本质上是将原始的椭球状分布的数据，先进行线性变换（旋转+缩放），把它变成一个标准的正圆（球）分布，然后再去算欧氏距离。

------

### 4. 在聚类中的应用

#### A. 对比 K-Means

- **K-Means**：假设 $\Sigma = I$（单位矩阵）。它认为所有簇都是**圆球形**的。如果数据簇是细长的，K-Means 效果会很差。
- **引入马氏距离**：如果我们允许每个簇有自己的协方差矩阵 $\Sigma_k$，那么聚类算法就能适应**任意椭球状**的簇。
  - 这实际上就是 **高斯混合模型 (GMM, Gaussian Mixture Model)** 的核心思想。

#### B. 异常检测 (Anomaly Detection)

判断一个点是不是离群点，不能只看绝对距离。

- 一个点虽然离中心有点远，但如果它在数据分布的“长轴”方向上（大家都很分散），那它可能很正常。
- 但如果它在“短轴”方向上偏离了一点点（大家都很集中），那它可能就是异常点。
- 马氏距离能完美衡量这种“统计学上的偏离程度”。

------

### 5. 与 Metric Learning 的关系

还记得我们刚才讲 Metric Learning 时提到的“学习一个矩阵”吗？

在度量学习中，如果我们学习一个参数矩阵 $M$ 来计算距离：



$$D(x, y) = \sqrt{(x-y)^T M (x-y)}$$

- 如果 $M = I$（单位矩阵），这就是**欧氏距离**。
- 如果 $M = \Sigma^{-1}$（统计出的协方差逆矩阵），这就是**马氏距离**。
- 在深度 Metric Learning 中，我们不直接算 $\Sigma$，而是让神经网络去**学习**出一个最优的 $M$（或者学习一个变换 $L$，使得 $M=L^T L$），让变换后的空间分类效果最好。

总结：

马氏距离就是**“带统计学视角的欧氏距离”。它不再机械地测量绝对距离，而是测量点相对于分布中心的“标准差倍数”**。