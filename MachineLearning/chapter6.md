# 集成学习



**集成学习（Ensemble Learning）** 是机器学习中“三个臭皮匠，顶个诸葛亮”哲学的完美体现。

它的核心理念不再是试图训练一个完美的“超级模型”，而是训练一堆平庸的“弱模型”，然后让它们组合起来投票或接力。

集成学习主要分为两大流派：

1. **Bagging (并行)**：大家各学各的，最后投票。代表作：**随机森林 (Random Forest)**。
2. **Boosting (串行)**：大家接力学，后面的人专门改前面人犯的错。代表作：**GBDT, XGBoost**。

我们先重点讲讲基于 Bagging 的 **随机森林**。

------

## Bagging

**Bagging** 是 **B**ootstrap **agg**regat**ing**（自助采样聚合）的缩写。

**核心痛点：决策树的方差太大**

我们在上一节讲过，单棵决策树（尤其是完全生长的树）非常容易过拟合。

- 如果你换了一组训练数据，生成的树可能长得完全不一样。
- 这叫做**高方差 (High Variance)**。

**解决方案：平均化**

统计学告诉我们：如果 $X_1, X_2, \dots, X_n$ 是独立同分布的随机变量，方差为 $\sigma^2$，那么它们的均值 $\bar{X}$ 的方差会降低到 $\frac{\sigma^2}{n}$。

也就是说：取平均值可以极大地降低波动（方差）。

---

**算法步骤**

1. **自助采样 (Bootstrap Sampling)**：
   - 从只有 $N$ 个样本的原始训练集中，**有放回地**随机抽取 $N$ 次。
   - 结果：生成了一个新的训练集（大小仍为 $N$），但里面有些样本重复出现了，有些样本没出现（约 36.8% 的样本没被抽到，称为 Out-of-Bag 数据）。
   - **目的**：让每棵树“看到的”数据不一样，增加多样性。
2. **并行训练**：
   - 用这些不同的子数据集，分别训练 $K$ 棵决策树。
3. **聚合 (Aggregating)**：
   - **分类**：所有树进行投票 (Voting)，少数服从多数。
   - **回归**：所有树的预测值取平均 (Averaging)。

------

### 随机森林 (Random Forest)

随机森林是 Bagging 的进化版。单纯的 Bagging 有一个问题：**树之间太像了（相关性太高）。**

假如数据里有一个特征（比如“收入”）预测能力极强，那么不管怎么采样，训练出来的所有树都会首选“收入”作为根节点。这导致所有树长得差不多，取平均后效果提升有限。

随机森林为了打破这种相关性，引入了**双重随机性**：

**样本随机 (Row Subsampling)**

就是上面提到的 Bootstrap，每棵树用的样本不同。

**特征随机 (Feature Subsampling) —— 关键创新**

在决策树分裂的每一个节点：

- **普通树**：遍历**所有** $d$ 个特征，选最好的。
- **随机森林**：先随机抽取 $k$ 个特征（通常 $k = \sqrt{d}$ 或 $\log_2 d$），只在这 $k$ 个特征里选最好的。

物理意义：

这强迫某些树不能使用“收入”这个最强特征，只能利用次优特征（比如“学历”）去寻找规律。

虽然这让单棵树变弱了（偏差增加），但因为树与树之间的相关性大幅降低，整体模型的方差急剧下降，最终的总误差反而更低。

**Out-of-Bag (OOB) 评估**

随机森林有一个独特的优势：**不需要专门划分验证集**。

- 因为每棵树都只用了 ~63.2% 的数据训练，剩下的 ~36.8% 数据（OOB 数据）天然就可以用来做验证。
- 我们可以直接用 OOB 误差来评估模型的泛化能力，这在小样本数据上非常宝贵。

------



## Boosting

**Boosting**是目前竞赛圈（Kaggle）的霸主。

如果说 Random Forest 是“民主投票”（大家独立发表意见，最后汇总），那么 Boosting 就是**“接力赛”**（每个人都在修正前一个人的错误）。

Boosting 家族最核心的演进路线是：**AdaBoost (只关注分错的样本) $\rightarrow$ GBDT (拟合残差) $\rightarrow$ XGBoost (二阶泰勒展开 + 正则化)**。

---

### AdaBoost

**AdaBoost (Adaptive Boosting)** 是 Boosting 家族的“鼻祖”。虽然现在大家打比赛更多用 GBDT/XGBoost，但 AdaBoost 是第一个在理论上被证明有效的 Boosting 算法，它的思想非常经典且优美。

如果说 GBDT 是通过“拟合残差”来不断修正数值误差，那么 AdaBoost 的核心逻辑就是**“错题本”策略**。

它不关心具体的数值差了多少，它只关心：**这道题你做对了吗？如果没做对，下次考试这道题的分值加倍！**

以下是 AdaBoost 的详细解析：

------

**核心思想：两个“权重”的博弈**

AdaBoost 的全称是 **Adaptive Boosting（自适应增强）**。它的“自适应”体现在它能自动调整数据的权重。

整个算法主要是在调整两个东西：

1. **样本的权重 (Sample Weights)**：每一轮训练前，给“难啃”的样本更高的权重，强迫模型去关注它们。
2. **模型的权重 (Classifier Weights)**：每一轮训练后，给表现好的模型更大的话语权，表现差的模型话语权减小。

------

**算法流程详解**

假设我们做二分类问题（-1 和 +1）。

**Step 1: 初始化权重**

一开始，所有样本都是平等的。如果有 $N$ 个样本，每个样本的权重 $w_i = \frac{1}{N}$。

**Step 2: 迭代训练（假设进行 M 轮）**

A. 训练弱分类器

用当前带权重的样本训练一个简单的模型 $h_m(x)$。

- *注意：AdaBoost 通常使用最简单的“决策桩” (Decision Stump)，也就是只有一层的决策树，切一刀就完事。*

B. 计算错误率 ($\epsilon_m$)

看看这个模型 $h_m(x)$ 在加权样本上做错了多少。

$$\epsilon_m = \text{分错样本的权重之和}$$

C. 计算该模型的发言权 ($\alpha_m$)

这是 AdaBoost 最经典的公式：

$$\alpha_m = \frac{1}{2} \ln \left( \frac{1 - \epsilon_m}{\epsilon_m} \right)$$

- **直观理解**：
  - 如果错误率 $\epsilon_m$ 很低（模型很准），$\frac{1-\epsilon}{\epsilon}$ 就会很大，$\alpha_m$ 就很大（在这个模型上投重注）。
  - 如果错误率 $\epsilon_m = 0.5$（瞎猜），$\ln(1) = 0$，$\alpha_m = 0$（这个模型这就是废话，不听它的）。
  - 如果错误率 $\epsilon_m > 0.5$（反向预测），$\alpha_m$ 会变成负数（反着听它的）。

D. 更新样本权重 ($w_i$) —— 关键一步

为了下一轮训练，我们需要调整样本权重：

- **如果做对了**：权重变小。 $w_{new} = w_{old} \cdot e^{-\alpha}$
- **如果做错了**：权重变大。 $w_{new} = w_{old} \cdot e^{\alpha}$

最后归一化，让权重之和重新等于 1。

- *效果：上一轮分错的样本，在下一轮中会变得像气球一样膨胀，迫使下一个弱分类器必须把精力花在这些“大胖子”样本上。*

**Step 3: 强力组合**

经过 M 轮后，我们将所有弱分类器加权投票：

$$H(x) = \text{sign} \left( \sum_{m=1}^{M} \alpha_m h_m(x) \right)$$

------

### 三、 数学本质：指数损失函数

你可能会问：*为什么 $\alpha$ 要包含 $\ln$？为什么权重更新要乘 $e^{\alpha}$？*

这并不是拍脑袋定的。AdaBoost 的本质是在最小化指数损失函数 (Exponential Loss)：

$$L(y, f(x)) = e^{-y f(x)}$$

通过前向分步算法（Forward Stagewise Algorithm）推导，你会发现上述的所有更新公式，正是为了让这个指数损失函数下降得最快。

- **GBDT** 最小化的是 MSE 或 Log-Loss。
- **AdaBoost** 最小化的是 Exponential Loss。
- 这也解释了为什么 AdaBoost 主要用于分类，因为指数损失非常严厉地惩罚分类错误的点。

------

**优缺点总结**

**优点：**

1. **精度高**：能够把很多简单的弱分类器组合成精度极高的强分类器。
2. **参数少**：相比 GBDT/XGBoost，AdaBoost 不需要调太多参数，往往默认设置就不赖。
3. **不易过拟合**：这是一个神奇的特性，在训练误差降到 0 之后继续训练，测试误差往往还能继续下降（这与“间隔理论 Margin Theory”有关）。

致命缺点：对异常值 (Outliers) 极度敏感！

这是 AdaBoost 最大的软肋。

- **场景**：假设数据里有一个标记错误的“脏数据”（比如把一张狗的图标记成了猫）。
- **后果**：
  - 第一轮，模型分错了它。
  - 第二轮，它的权重变大。
  - ...
  - 第十轮，这个样本的权重已经大到离谱，其他所有正常样本加起来都没它重要。
  - **结果**：模型为了强行拟合这个错误的噪点，把整个分类面扭曲得不成样子，导致模型崩溃。

总结：

AdaBoost 是 Boosting 思想的奠基者，它的“错题本”逻辑非常直观。但在实际工程中，由于它对噪声数据缺乏抵抗力（不像 GBDT 可以通过 Huber Loss 等鲁棒损失函数来抗噪），现在在复杂数据竞赛中已经很少作为主力模型出现了。

---

### GBDT

我们重点讲解现代算法的基石——**GBDT (Gradient Boosting Decision Tree)**。

**核心直觉：基于“残差”的训练**

GBDT 的核心思想非常简单，简单到可以用小学数学来解释。

**场景**：我们要预测一个人的年龄，真实值是 **30 岁**。

1. **第一个模型 (Tree 1)**：
   - 它比较笨，预测是 **20 岁**。
   - **残差 (Residual)** = 真实值 - 预测值 = $30 - 20 = \mathbf{10}$。
   - *潜台词：模型 1 说“我还差 10 岁没预测出来”。*
2. **第二个模型 (Tree 2)**：
   - 它的目标**不再是预测 30 岁**，而是去预测模型 1 剩下的那个 **残差 10**。
   - 假设它预测出了 **6**。
   - 新的预测值 = $20 + 6 = 26$。
   - **新残差** = $30 - 26 = \mathbf{4}$。
3. **第三个模型 (Tree 3)**：
   - 它的目标是去预测剩下的 **残差 4**。
   - 假设它预测出了 **3**。
   - 新的预测值 = $20 + 6 + 3 = 29$。
   - **新残差** = $\mathbf{1}$。
4. **最终模型**：
   - $F(x) = \text{Tree}_1(x) + \text{Tree}_2(x) + \text{Tree}_3(x) + \dots$
   - 预测结果逼近 30。

**结论**：GBDT 中的每一棵树（弱学习器）学的是**“之前所有树预测结果的累加和”与“真实值”之间的差距**。

------

**为什么要叫“梯度”提升 (Gradient Boosting)？**

你可能会问：*“这不就是拟合残差吗？跟‘梯度’有什么关系？”*

当损失函数是**均方误差 (MSE)** 时，**残差恰好等于负梯度**。

数学推导：

假设损失函数是 MSE：

$$L(y, F(x)) = \frac{1}{2}(y - F(x))^2$$

我们对当前的预测值 $F(x)$ 求导（注意是把 $F(x)$ 看作变量，而不是 $w$）：

$$\frac{\partial L}{\partial F(x)} = -(y - F(x))$$

所以：

$$\text{负梯度} = - \frac{\partial L}{\partial F(x)} = y - F(x) = \text{残差}$$

深刻的洞察：

如果我们的损失函数不是 MSE（比如是用于分类的交叉熵损失，或者是用于排名的 Pairwise Loss），直接算“残差”就很难定义了。

但是，“负梯度”永远代表着函数下降最快的方向。

GBDT 的通用逻辑：

不管你用什么损失函数，每一棵新树 $h_m(x)$ 的任务，就是去拟合上一轮损失函数的负梯度。

$$r_{im} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x) = F_{m-1}(x)}$$

这就是 GBDT 名字的由来：它是在**函数空间 (Function Space)** 里做梯度下降。

------

**GBDT 的完整算法流程**

1. 初始化：

   先给一个初始值 $F_0(x)$（通常是所有标签的平均值，或者 0）。

2. **循环生成 M 棵树**（for $m = 1$ to $M$）：

   - A. 计算伪残差 (Pseudo-residuals)：

     对每个样本 $i$，计算当前负梯度 $r_{im}$。

   - B. 训练弱学习器：

     用训练集 $\{(x_i, r_{im})\}$ 训练一棵 CART 回归树（注意：这里不管原任务是分类还是回归，拟合梯度的树永远是回归树）。

   - C. 计算叶子节点值：

     计算这棵树每个叶子节点的最佳输出值 $\gamma_m$（这一步是为了让损失尽可能小）。

   - D. 更新模型：

     $$F_m(x) = F_{m-1}(x) + \eta \cdot h_m(x)$$

     注意这里引入了一个学习率 (Learning Rate) $\eta$（通常是 0.01 或 0.1）。这就叫 Shrinkage（缩减）。我们不希望一步走到位，而是小步快跑，防止过拟合。

3. 输出最终模型：

   $$F_M(x) = F_0(x) + \sum_{m=1}^{M} \eta \cdot h_m(x)$$

------

**既然都是树，GBDT 和 随机森林 的区别？**

这是面试中最高频的问题，我们可以从几个维度对比：

| **维度**     | **Random Forest (随机森林)** | **GBDT (梯度提升树)**                   |
| ------------ | ---------------------------- | --------------------------------------- |
| **组成**     | **并行** (Parallel)          | **串行** (Sequential)                   |
| **单树特点** | **深树** (过拟合的强模型)    | **浅树** (欠拟合的弱模型，深度通常 3-8) |
| **核心目的** | **降低方差** (Variance)      | **降低偏差** (Bias)                     |
| **对异常值** | 不敏感 (抗噪)                | **敏感** (会拼命去拟合异常值的残差)     |
| **训练速度** | 快 (可多核并行)              | 慢 (后一棵树依赖前一棵)                 |
| **适用场景** | 数据噪点多、防止过拟合       | 追求高精度、数据较干净                  |

------

**XGBoost：GBDT 的“完全体”**

GBDT 理论很完美，但早期的工程实现比较慢。直到 **XGBoost (eXtreme Gradient Boosting)** 的出现，彻底引爆了竞赛圈。

XGBoost 本质上还是 GBDT，但它做了大量的数学和工程优化。其中最核心的数学改进是：**二阶泰勒展开**。

普通 GBDT 只用到了一阶导数（梯度）：

$$L(y, F_{m-1} + h) \approx L(y, F_{m-1}) + g \cdot h$$

（$g$ 是梯度，$h$ 是新树）

XGBoost 用到了二阶导数（海森矩阵，Hessian）：

$$L(y, F_{m-1} + h) \approx L(y, F_{m-1}) + g \cdot h + \frac{1}{2} \mathbf{H} \cdot h^2$$

**为什么要用二阶导？**

1. **更准**：就像牛顿法比梯度下降法收敛更快一样，二阶信息能让我们更精准地找到下降方向。
2. **自定义损失函数更方便**：只要你的损失函数能求出一阶导 ($g$) 和二阶导 ($h$)，你就可以把它们丢进 XGBoost 里训练，完全不用改动算法核心代码。

此外，XGBoost 还在目标函数里显式地加上了**正则化项**（控制叶子节点个数和权重），相当于在 GBDT 内部自动做了剪枝。

------

**总结**：

- **Boosting** 是一群差生互相补课，最后变成优等生。
- **GBDT** 利用**梯度下降**的思想，每一棵树都在拟合上一轮的负梯度（残差）。
- **XGBoost** 是 GBDT 的工程加强版，利用二阶导数和正则化，跑得又快又准。

