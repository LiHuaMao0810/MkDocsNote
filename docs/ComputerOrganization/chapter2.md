---
comments : true
---



# Chapter2 计算机的算数运算



要理解计算机如何进行算术运算，首先要明白计算机是个“一根筋”的机器：它**只会做加法**，且**存储空间有限**。为了用加法代替减法，并处理正负号，人类设计了一套精妙的编码系统。

------

## 1. 整数的存储编码

### 原码 (Sign-Magnitude)

- **规则**：最高位为符号位（0正1负），其余位表示数值的绝对值。
- **例子**（以 8 位为例）：
  - $+1 = [0000 0001]$
  - $-1 = [1000 0001]$
- **缺点**：**0 有两种表示**（$+0$ 和 $-0$），且加减法运算逻辑不统一，硬件设计极其复杂。

### 反码 (Ones' Complement)

- **规则**：正数与原码相同；负数符号位不变，数值位逐位取反。
- **例子**：
  - $-1$ 的反码 = $[1111 1110]$
- **意义**：它是原码转补码的中间状态，目前极少直接用于存储。

### 补码 (Two's Complement) —— **计算机的核心**

- **规则**：正数与原码相同；负数在反码的基础上 **+1**。
- **例子**：
  - $-1$: 原码 `10000001` $\to$ 反码 `11111110` $\to$ 补码 `11111111`
- **为什么要用补码？**
  1. **统一加减法**：$A - B$ 可以看作 $A + (2^n -B) \mod 2^n$。使用补码运算，符号位直接参与计算，结果依然正确。
  2. **消灭了 $-0$**：$0$ 只有一种表示 `0000 0000`。
  3. **多出一个数**：8位补码可以表示 $-128$ 到 $127$（比原码多表示一个 $-128$）。

### 移码 (Excess/Biased)

- **规则**：在数值上加上一个固定的“偏置常数”（Bias）。通常用于表示浮点数的**阶码**。
- **例子**：8 位移码偏置 127，则 $0$ 表示为 `0111 1111`（即 $0+127$）。
- **好处**：保持了数轴的线性顺序，**全 0 为最小值，全 1 为最大值**，方便硬件直接比较大小。

### 计算补码

**“绝对值按位取反 + 1”** 确实是计算补码最通用、最不会出错的方法，它背后的本质正是**同余（取模）**。

在数学上，按位取反（逻辑非）操作其实是用全 $1$ 的数减去原数。

对于 $n$ 位二进制数，全 $1$ 的数就是 $2^n - 1$。

当你把绝对值 $X$ 按位取反时，你实际上是在计算：

$$\text{反码} = (2^n - 1) - X$$

当你再 $+1$ 时：

$$\text{补码} = (2^n - 1) - X + 1 = \mathbf{2^n - X}$$

这正是取模的定义！ 在 $2^n$ 的模系统中，$-X$ 的等价正数就是 $2^n - X$。

比如在时钟（模 12）上，倒拨 3 小时（$-3$）和顺拨 9 小时（$12 - 3 = 9$）效果是一样的。

> [!tip]
>
> 分享一个比“取反加一”更快的**眼神观察法**，它不需要你真的去做加法。
>
> **规则：从右往左看，找到第一个“1”，保持这个“1”和它右边的“0”不变，把这个“1”左边的所有位全部取反。**
>
> - **例子：计算 -8 (8位二进制)**
>   1. 8 的原码：`0000 1000`
>   2. 从右往左看，第一个 `1` 在第四位。
>   3. 保持右边的 `1000` 不变。
>   4. 把左边的 `0000` 全部取反，变成 `1111`。
>   5. 结果：`1111 1000`（这就是 -8 的补码）。
>
> 这个方法其实就是“按位取反 + 1”在逻辑上的合并，非常适合手动草稿。
>
> 学过数逻的同学可能知道，数逻里面补码电路就可以这么设计



------

## 2. IEEE 754 浮点数表示

如果说补码是为了解决整数的正负问题，IEEE 754 就是为了解决“天文数字”和“微观数字”的存储问题。它类似**科学计数法**。

一个浮点数被拆分为三部分：

$$V = (-1)^S \times M \times 2^E$$

### 结构组成（以 32 位单精度 float 为例）：

1. **符号位 (Sign, 1 bit)**：0 代表正，1 代表负。
2. **阶码 (Exponent, 8 bits)**：
   - 使用**移码**存储。偏置值（Bias）为 **127**。
   - 如果你看到的阶码二进制值是 128，实际指数 $E = 128 - 127 = 1$。
3. **尾数 (Fraction/Mantissa, 23 bits)**：
   - 采用规格化表示，即隐藏开头的“1.”。例如 $1.011...$ 只存储 $011...$。
   - 这样可以多省出 1 位的精度。

> [!example] 将 $5.0$ 转换为浮点数
>
> 1. **转二进制**：$5.0 = 101_2$
> 2. **规格化**：$1.01 \times 2^2$
> 3. **确定参数**：
>    - 符号 $S = 0$
>    - 指数 $E = 2$，存储值 = $2 + 127 = 129$ (`1000 0001`)
>    - 尾数 $M = 01$（后面补 0）
> 4. **最终存储**：`0 | 10000001 | 01000000000000000000000`

------

### **特殊数值**

IEEE 754 还规定了一些特殊情况，方便程序处理错误：

- **阶码全 0，尾数全 0**：表示 $\pm 0$。
- **阶码全 1，尾数全 0**：表示 $\pm \infty$ (无穷大)。
- **阶码全 1，尾数非 0**：表示 **NaN** (Not a Number，如 $0 \div 0$ 的结果)。

---

### **非规格数**

在 IEEE 754 标准中，**非规格化数（Denormalized numbers / Subnormal numbers）** 的引入是为了解决一个致命的问题：**渐进式下溢（Gradual Underflow）**。

如果没有非规格化数，浮点数在靠近 $0$ 的时候会突然“跳水”，导致巨大的计算误差。

![image-20251230171309306](./assets/image-20251230171309306.png)

> [!tip]
>
> 为什么需要非规格化数？（填补“零点真空”）
>
> 在**规格化数**（Standard Normalized numbers）的规定下：
>
> - 尾数前面必须隐含一个 `1.`。
> - 指数位（阶码）不能全为 $0$。
>
> 这会导致一个现象：**最小的规格化正数**是多少？
>
> - 阶码存储值为 `1`（即实际指数为 $1-127 = -126$）。
> - 尾数全为 $0$（隐含 $1.0$）。
> - 数值为：$1.0 \times 2^{-126}$。
>
> **问题来了**：在 $0$ 和 $1.0 \times 2^{-126}$ 之间，存在一个巨大的“真空地带”。如果两个很小的数相减，结果落在这个区间，硬件只能将其强制转为 $0$。这种“突然变成 $0$”的情况会引发严重的数学错误。



当**阶码（Exponent）全为 0** 时，这个数就进入了“非规格化”模式。它的规则发生了两个关键变化：

1. **隐含位由 `1.` 变为 `0.`**：尾数不再是 $1.f...$，而是 $0.f...$。
2. **指数固定为 $-126$（对于单精度）**：注意！虽然阶码存储值是 $0$，但它的实际指数**不是** $0 - 127 = -127$，而是强制规定为 **$-126$**。

------

为什么要强制规定指数为 $-126$？

这是为了实现从规格化数到非规格化数的**平滑平移**。

我们可以对比一下：

- **最小的规格化数**：$1.000... \times 2^{-126}$
- **最大的非规格化数**：$0.111... \times 2^{-126}$（尾数全为 $1$）

你看，它们的指数都是 $-126$！

通过这种设计，非规格化数就像是一把“接力棒”，在数值小于 $1.0 \times 2^{-126}$ 时，通过减小尾数开头的 $0.$ 后面的有效位，一点点地靠近 $0$。

------

### 浮点数的完整分类表

现在我们可以总结出 IEEE 754 的全貌了：

| **阶码 (E)**         | **尾数 (M)** | **表示含义**   | **数值计算公式**                       |
| -------------------- | ------------ | -------------- | -------------------------------------- |
| `00...0`             | `00...0`     | **真 0**       | $\pm 0$                                |
| `00...0`             | **非全 0**   | **非规格化数** | $(-1)^S \times (0.M) \times 2^{-126}$  |
| `01..01` 到 `11..10` | 任意         | **规格化数**   | $(-1)^S \times (1.M) \times 2^{E-127}$ |
| `11...1`             | `00...0`     | **无穷大**     | $\pm \infty$                           |
| `11...1`             | **非全 0**   | **NaN**        | 不是一个数字（无效运算）               |

------

> [!note]
>
> 代价是什么？
>
> 虽然非规格化数在数学上很完美，但在早期计算机硬件中，处理非规格化数的逻辑非常复杂，速度通常比普通浮点运算**慢几十甚至上百倍**。
>
> 因此，在一些对性能要求极高的领域（如 3D 游戏渲染、深度学习），程序员有时会开启一个叫 **Flush-to-Zero (FTZ)** 的模式，直接把非规格化数当 $0$ 处理，牺牲一点点精度来换取巨大的速度提升。

**总结一下：**

- **规格化数**：$1.xxxxx \times 2^{Exp}$，用于处理常规数值。
- **非规格化数**：$0.xxxxx \times 2^{-126}$，用于在靠近 $0$ 的极小区间内提供“缓冲”，避免计算结果突然崩塌。

---

## 3. IEEE 754 浮点数运算

浮点数的运算比整数复杂得多，因为整数的权值是固定的，而浮点数就像是带了“缩放比例”的数字。在进行加法或乘法时，硬件必须像拆解零件一样，先处理指数，再处理尾数。

------

### 1. 浮点数加法：必须先“对齐”

浮点数加法最麻烦的地方在于：**指数不同，尾数不能直接相加**。这就像你不能直接把 $1.2 \times 10^2$ 和 $3.4 \times 10^3$ 的尾数相加（$1.2+3.4$）一样，必须先统一指数。

**步骤一：对阶 (Alignment)**

- **原则**：**小阶向大阶看齐**。
- **原因**：右移尾数会导致低位丢失（精度下降），而左移尾数会导致高位溢出，所以只能选择右移。
- **操作**：计算指数差 $\Delta E = E_1 - E_2$。将指数较小的数的尾数向右移 $\Delta E$ 位，指数加 $\Delta E$。

**步骤二：尾数求和 (Addition)**

- 将对齐后的两个尾数（记得算上隐含的“1”）按整数补码方式相加。

**步骤三：规格化 (Normalization)**

- 如果结果不是 $1.x...$ 的形式，需要调整。
- **右规**：如果求和导致进位（如变成 $10.x...$），尾数右移，指数加 1。
- **左规**：如果相减导致高位变 0（如变成 $0.001...$），尾数左移，指数相应减小。

**步骤四：舍入 (Rounding)**

- 移位过程中掉出去的位，需要根据 IEEE 754 规定的模式（如“向最接近的值舍入”）进行补偿。

**步骤五：溢出检测**

- 检查最终指数是否超出了 8 位阶码能表示的范围（上溢到 $\infty$ 或下溢到 $0$）。

![image-20251230171949293](./assets/image-20251230171949293.png)

------

### 2. 浮点数乘法：简单但“宽”

相比加法，乘法反而不需要对阶，因为它遵循指数相加、尾数相乘的逻辑。

**步骤一：计算符号位**

- 使用异或运算：$S_{res} = S_1 \oplus S_2$（同号得正，异号得负）。

**步骤二：指数相加**

- **注意**：由于指数是移码存储（带 Bias），直接相加会重复计算偏移量。
- **公式**：$E_{res} = (E_1 - 127) + (E_2 - 127) + 127 = E_1 + E_2 - 127$。

**步骤三：尾数相乘**

- 两个 24 位（含隐含位）的尾数相乘，得到一个 48 位的积。
- 这是硬件中最耗资源的部分，通常需要大量的乘法阵列。

**步骤四：规格化、舍入与溢出检测**

- **规格化**：由于 $1.x \times 1.y$ 的结果范围在 $[1.0, 4.0)$ 之间，结果最多只需要右规一次（如果积 $\ge 2.0$）。
- **舍入**：将 48 位的中间结果截断回 23 位。

------

### 3. 为什么乘法有时比加法“快”？

在现代 CPU 中，你可能会发现浮点乘法的延迟并不比加法高多少，甚至在某些架构下吞吐量更高。原因有二：

1. **无需对阶**：乘法省去了“比较指数-循环移位”的繁琐预处理。
2. **FMA 指令 (Fused Multiply-Accumulate)**：现代处理器通常支持 $A \times B + C$ 一次性完成，只进行一次舍入。这不仅提高了速度，还减少了精度损失。

---

### 4. 舍入模式控制

在 IEEE 754 标准中，舍入（Rounding）是连接“无限精度的数学运算”与“有限位数的计算机存储”之间的桥梁。当浮点运算产生的结果位数（如乘法产生的 48 位尾数）超过了寄存器能定义的位数（如 23 位）时，就必须进行舍入。

IEEE 754 定义了五种主要的舍入模式，分为**常用模式**和**定向模式**两类。

------

 **常用模式（最近舍入）**

这是大多数编程语言（如 C, Python, Java）默认采用的模式。

① 就近舍入，向偶数舍入 (Round to Nearest, Ties to Even)

这是 IEEE 754 的**默认模式**，也常被称为“银行家舍入”。

- **规则**：舍入到最接近的数值。
- **关键点（Ties）**：如果结果正好处于两个数值的正中间（例如想保留整数，结果是 1.5 或 2.5）：
  - 向**偶数**方向舍入。
  - `1.5` $\rightarrow$ `2`
  - `2.5` $\rightarrow$ `2`
- **为什么要这么做？** 如果一律“四舍五入”（0.5 总是向上），在处理海量数据加法时，结果会产生明显的正向偏差（统计漂移）。向偶数舍入使 0.5 有一半概率向上，一半概率向下，从而抵消误差。

② 就近舍入，向远离 0 舍入 (Round to Nearest, Ties to Away from Zero)

- **规则**：舍入到最接近的数值。
- **关键点**：如果正好在中间，选择绝对值更大的那个（即传统的“四舍五入”）。
  - `1.5` $\rightarrow$ `2`
  - `-1.5` $\rightarrow$ `-2`

------

**定向模式 (Directed Rounding)**

这些模式通常用于区间算术（Interval Arithmetic），通过确定结果的上界和下界来控制计算误差。

③ 向零舍入 (Round toward Zero / Truncate)

- **规则**：直接截断多余的位数，不看大小。
- **效果**：正数向下变小，负数向上变大（绝对值都在变小）。
  - `1.9` $\rightarrow$ `1`
  - `-1.9` $\rightarrow$ `-1`

④ 向正无穷舍入 (Round toward $+\infty$ / Ceiling)

- **规则**：结果向数轴右侧靠拢。
- **效果**：
  - `1.1` $\rightarrow$ `2`
  - `-1.9` $\rightarrow$ `-1`

⑤ 向负无穷舍入 (Round toward $-\infty$ / Floor)

- **规则**：结果向数轴左侧靠拢。
- **效果**：
  - `1.9` $\rightarrow$ `1`
  - `-1.1` $\rightarrow$ `-2`

------

**舍入过程中的三个关键位**

为了实现精准的舍入，硬件在运算时会额外保留 3 位信息，被称为 **GRS 位**：

1. **G (Guard bit)**：保护位，紧跟在有效数位之后。
2. **R (Round bit)**：舍入位，在 G 之后。
3. **S (Sticky bit)**：粘滞位。只要 R 之后有任何一位是 1，S 就被置为 1。

这三位的作用：

它们帮助硬件判断中间结果是“刚好 0.5”、“小于 0.5”还是“大于 0.5”。

- 如果 $GRS = 100$：说明恰好在中间（Ties）。
- 如果 $GRS > 100$：说明大于中间值，应进位。
- 如果 $GRS < 100$：说明小于中间值，应舍弃。

------

