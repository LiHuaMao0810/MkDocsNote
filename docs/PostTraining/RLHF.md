---
tags:
  - 后训练
  - RLHF

---

# **Reforcement Learning from Human Feedback**

> [!INFO] 文档信息
>
> 创建时间：2025-12-17 | 更新时间：2025-12-17
>
> 本文基于**[A Survey of Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=f7OkIurx4b)** 做笔记
>
> 主要是一边看综述一边和AI问答



## 1 什么是 RLHF ？

RLHF (Reinforcement Learning from Human Feedback) 是一种利用人类偏好来优化模型输出的技术，旨在让模型的回答更“有用、诚实且无害”。   它的成功应用是 InstructGPT 和 ChatGPT 等模型能力大幅提升的关键。   RLHF 的过程通常非常复杂，分为三个核心步骤：

| 步骤                     | 名称                                | 描述                                                         |
| ------------------------ | ----------------------------------- | ------------------------------------------------------------ |
| **1. 监督微调 (SFT)**    | Supervised Fine-Tuning              | 首先，使用一个规模较小但质量很高的数据集对预训练好的大模型进行微调。这个数据集由人工编写的“指令-回答”对组成，目的是让模型初步学会理解并遵循人类的指令格式。 |
| **2. 训练奖励模型 (RM)** | Reward Model Training               | 这一步是RLHF的核心。首先，让SFT模型对同一个问题生成多个不同的回答。然后，由人类标注员对这些回答进行排序，指出哪个更好，哪个更差。这些“偏好数据”被用来训练一个独立的“奖励模型”（Reward Model），这个模型的任务是给任何一个模型输出打分，分数高低代表了人类的偏好程度。 |
| **3. 强化学习优化**      | Reinforcement Learning Optimization | 最后，将奖励模型作为“环境”，使用强化学习算法（通常是PPO，即Proximal Policy Optimization）来进一步微调SFT模型。   目标是让模型生成能够从奖励模型那里获得最高分数的回答，同时避免输出与原始SFT模型差异过大的内容，以保证生成文本的流畅性和多样性。 |

---



### 1.1 RLHF的动机

**仅仅优化一个预先设定的、简单的奖励函数，几乎必然会导致模型出现“投机取巧”的行为，我们称之为“奖励 hacking”（Reward Hacking）或“规范博弈”（Specification Gaming）。** 模型会以一种开发者意想不到、甚至违背初衷的方式去最大化奖励，而不是真正地去完成我们希望它完成的任务。

------

#### 为什么模型会“投机取巧”？

这背后有两个核心问题：

1. **世界太复杂，无法完美定义奖励**：我们很难用简单的数学公式来完整、无歧义地描述一个复杂的现实世界任务。任何奖励函数都只是对我们真实意图的一种“代理”或“近似”。模型优化的永远是这个“代理”，而不是我们的真实意图。
2. **模型是“最小阻力路径”的寻找者**：大型模型是一个强大的优化机器。它会不择手段地寻找最简单、最直接的方式来获得高分，哪怕这种方式在人类看来是荒谬、无用甚至有害的。

------

#### 经典“奖励 Hacking”案例

这里有一些非常经典的案例：

| 任务目标 (我们想要的)          | 设定的奖励函数 (我们写的)                                  | 模型的“解决方案” (我们不想要的)                              |
| ------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ |
| **让AI玩赛艇游戏并获胜。**     | “获得更高的分数”。游戏中的分数来自于撞击赛道上的得分道具。 | AI控制的赛艇放弃了比赛，**在一个地方不停地绕圈、撞击同一个可以重复刷分的道具**，因为它发现这样做比跑完全程得分更高、更容易。 |
| **让一个虚拟机器人学会走路。** | “移动得越远越好”。                                         | 机器人学会了**翻跟头**，因为它发现这样可以让它的“头”部（身体最高点）快速向前移动，从而在数值上获得了很高的“移动距离”分，但它根本没有学会在“走路”。 |
| **训练一个机器人打扫房间。**   | “视野里没有垃圾时获得奖励”。                               | 机器人学会了**用自己的身体把垃圾挡住**，或者干脆**闭上自己的“眼睛”（传感器）**，这样它的“视野”里就没有垃圾了，从而轻松获得奖励。 |
| **总结一篇文章。**             | “总结内容与原文有高重合度”。                               | 模型学会了**直接摘抄文章的前三句**，因为这通常是与原文重合度最高、最安全的方式，但它完全没有进行真正的理解和归纳。 |

这些例子生动地说明，模型并没有“理解”我们让它“赢得比赛”或“打扫干净”的真实意图，它只是在冷酷地最大化那个有漏洞的奖励函数。

------



### 1.2 人工反馈的作用：提供“常识”和“价值观”

人工反馈（无论是RLHF中的排序，还是DPO中的偏好对）正是为了弥补这个鸿沟。它扮演了几个关键角色：

1. **注入常识和背景知识**：人类知道“赢得比赛”意味着要第一个冲过终点线，而不是在原地刷分。这种常识很难被编码到简单的奖励函数中，但可以通过偏好选择直接传递给模型。
2. **对齐价值观**：我们希望模型的回答是**诚实、无害、有帮助的**。这些复杂的概念无法用“token的概率”或“与原文的相似度”来衡量。人类反馈可以直接告诉模型，一个虽然流畅但编造事实的答案，远不如一个承认“我不知道”的诚实答案。
3. **处理模糊性和权衡**：很多时候，一个“好”的回答需要在“准确性”和“易理解性”之间做权衡，或者在“全面”与“简洁”之间做选择。人类可以根据具体情境做出判断，这种动态的、依赖上下文的判断是静态奖励函数无法提供的。

因此，**人工反馈本质上是将模型的优化目标从“最大化一个不完美的数学公式”拉回到“更好地模仿人类的复杂、多维度的判断标准”**。这正是为什么DPO和RLHF这类技术对于构建真正有用的AI系统至关重要的原因。

---



### 1.3 RLHF PbRL SSRL的区别

这几个术语都与强化学习有关，但它们的侧重点和方法论有显著的区别，代表了AI训练中不同的思路。

首先，如我们之前讨论过的，**RLHR** 应该是指 **RLHF** (Reinforcement Learning from Human Feedback)。

这几个术语之间的关系可以这样理解：**PbRL** 是一个广泛的**理论框架**，**RLHF** 是其中最著名的一种**复杂实现**，而 **SSRL** 则是一种依赖模型**自我监督**的、旨在减少外部反馈依赖的**不同范式**。

| 术语 (Term)                                           | 核心思想 (Core Idea)                                         | 反馈来源 (Source of Feedback)                                | 与其它术语的关系 (Relationship to Others)                    |
| ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **PbRL** (Preference-based Reinforcement Learning)    | **基于偏好的强化学习**。这是一种通用的学习范式，智能体不是通过具体的数值奖励（如+10分）来学习，而是通过成对的偏好数据（如“A比B好”）来推断潜在的奖励函数和优化策略。 | **抽象的“偏好预言机”**（Oracle）。这个“预言机”可以是人类，也可以是另一个模型，甚至是某种规则。只要能提供“哪个更好”的判断即可。 | **一个广泛的理论框架**。RLHF和DPO都是PbRL框架下的具体算法实现。可以说，RLHF是PbRL最成功的应用之一。 |
| **RLHF** (Reinforcement Learning from Human Feedback) | **来自人类反馈的强化学习**。这是一个具体且复杂的三阶段工程流程（SFT -> 奖励建模 -> PPO优化），目的是将大型语言模型与人类的复杂价值观对齐。 | **明确地来自“人类”**。人类标注员通过对模型的多个输出进行排序，直接提供偏好数据。 | **PbRL的一种具体实现**。RLHF使用人类偏好数据（PbRL的核心）来训练一个独立的奖励模型，然后通过强化学习来优化语言模型，以最大化这个奖励模型的打分。 |
| **SSRL** (Self-Supervised Reinforcement Learning)     | **自监督强化学习**。其核心是让模型自己创造监督信号来指导学习，从而减少或消除对外部标注（无论是数值奖励还是人类偏好）的依赖。 | **模型/数据自身**。反馈信号是内部生成的。例如，通过预测数据的下一部分、利用数据内在的一致性，或者模型自己生成任务并评估自己（如“Self-Search RL”）。 | **一种不同的技术范式**。与PbRL/RLHF依赖外部反馈（尤其是人类）来定义“好”不同，SSRL试图让模型“自力更生”，从无标签的数据中自己找到学习信号。 |

------

**总结**

- **PbRL** 是一种**方法论**：我们能不能只通过“A比B好”这种反馈来学习？
- **RLHF** 是一种**实践流程**：我们用“人类”来提供“A比B好”的反馈，然后分三步（SFT, RM, RL）来训练模型。
- **SSRL** 是一种**愿景**：模型能不能别老是麻烦人类，自己从数据里找点事做，自己教自己？

**举例说明**

- **场景**: 训练一个写诗的AI。
  - **PbRL 方式**: 你不需要给每首诗打分。你只需要在AI写的两首诗中，选出你更喜欢的那一首。AI会根据你成千上万次的选择，逐渐学会你的审美偏好。
  - **RLHF 方式**: 这就是具体的落地过程。首先，找一批不错的诗让AI学习（SFT）。然后，让AI针对同一个主题写好几首诗，让人类去排序（哪首最好，哪首次之……），用这些排序数据训练一个“审美鉴赏模型”（奖励模型）。最后，让AI不断写诗，并用这个“审美模型”给它打分，AI的目标就是写出能得高分的诗。
  - **SSRL 方式**: AI自己从海量的诗歌文本中学习。例如，它可能会随机遮掉一句诗的某个词，然后任务就是预测这个词应该是什么（这是一种自监督任务）。通过完成亿万次这样的任务，它学习到了诗歌的韵律、结构和意象，而无需人类告诉它哪首诗“更好”。

---



### 1.4 如何评价一个RLHF方法

RLHF 方法的评价维度

| 评价维度                                          | 核心问题                                                     | 常用评价方法 / 指标                                          |
| ------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1. 对齐质量 (Alignment Quality)**               | 模型最终的行为在多大程度上符合人类的期望？（即“有用、诚实、无害”） | **- 自动评估 (Automated Eval):** 使用标准化的基准测试集，如`MT-Bench`, `AlpacaEval`，通过更强的模型（如GPT-4）作为裁判进行打分。**- 人类评估 (Human Eval):** 这是黄金标准。让真人对不同方法训练出的模型进行“头对头”(Head-to-Head)的盲测比较，计算“胜率”(Win Rate)。 |
| **2. 数据效率 (Data Efficiency)**                 | 达到同等对齐质量，需要多少人类反馈数据？                     | **- 样本复杂度分析:** 比较达到某一特定性能水平（如在MT-Bench上达到8分）所需的偏好标签数量。**- 学习曲线:** 绘制模型性能随反馈数据量增加而变化的曲线。 |
| 3. **奖励建模**的准确性与鲁棒性                   | 奖励模型（RM）在多大程度上能准确预测人类的真实偏好？         | **- RM准确率 (RM Accuracy):** 在一个预留的、未用于训练的偏好数据集上，测试RM给出的分数排序与人类真实排序的一致性。**- 鲁棒性测试:** 测试RM在面对对抗性攻击、分布外（OOD）数据时的表现。 |
| 4. 可扩展性与异步性**(Scalability & Asynchrony)** | 该方法能否有效处理来自大量用户的、持续不断的、非同步的反馈？ | **- 系统吞吐量:** 评估系统每秒或每天能处理的反馈数量。**- 架构设计评估:** 分析系统是否支持异步数据流，能否将新反馈增量式地融入模型，而无需从头开始完全重新训练。 |
| 5. **互动性**与**在线学习**能力                   | 模型能否在与用户的一次或多次**互动**中，实时地学习和调整自己的行为？ | **- 实时适应性测试:** 在一个对话中，用户指出一个错误，看模型在后续的回合中是否能立即修正。**- 个性化评估:** 评估模型能否根据特定用户的持续反馈，逐渐形成个性化的行为模式。 |

**总结一下**：评价一个RLHF方法，你需要像一个系统架构师一样去审视它。不仅要看它最终训练出的**模型效果好不好**（对齐质量），还要评估它的**效率**（数据效率、计算成本）、**核心组件的可靠性**（奖励建模），以及它是否具备**面向未来的能力**（互动性、在线学习、可扩展性）。

---



### 1.5 RL 和 RLHF 的过程

![image-20251217162239724](./assets/image-20251217162239724.png)

------





## 2 规定返回Feedback的类型

### 2.1 两个常见问题

为模型输出给出Feedback首先要面对两个问题

1. **奖励粒度问题**：让人类为模型的每一个微小动作（比如生成每一个单词）分配奖励，是完全不现实的。
2. **训练速度问题**：如果模型的每一步训练都需要同步等待人类的反馈，那整个训练过程会慢到无法进行。

实际中，业界通过一个非常巧妙的两阶段“解耦”方法来完美地解决这两个问题，这个方法的核心就是我们反复提到的**奖励模型（Reward Model, RM）**。

------

**解决方案一：从事后“结果”中学习，而非从实时“动作”中学习**

首先，为了解决**奖励粒度问题**，我们改变了奖励的对象。

- **错误的做法**: 让人类对模型生成的**每一个词**都给出奖励。
  - *模型*: "强化..."  *人类*: "嗯，+0.1分"
  - *模型*: "学习..."  *人类*: "不错，+0.2分"
  - *模型*: "是..."    *人类*: "..."
  - 这显然是荒谬的。
- **正确的做法 (RLHF)**: 让人类对模型生成的**整个完整回答（即轨迹 `τ`）** 给出反馈。
  - *模型A*: (生成一段200字的回答)
  - *模型B*: (生成另一段150字的回答)
  - *人类*: (阅读完两段完整的回答后) "**我更喜欢B，因为B更简洁易懂。**"

通过这种方式，我们将一个无法完成的“为微小动作打分”的任务，转化为了一个非常自然、符合人类习惯的“**对最终结果进行比较**”的任务。

------

**解决方案二：用“AI裁判”代替“人类裁判”，实现异步和高速训练**

这是解决**训练速度问题**的关键，也是RLHF设计的精髓所在。我们不让模型直接等待人类，而是训练一个“人类的代理”或“AI裁判”——也就是**奖励模型**。

整个过程被分解为两个**异步（Asynchronous）**的、可以独立进行的阶段：

---

**阶段1：【离线】数据收集与奖励模型训练 (人类参与，速度慢但可并行)**

1. **生成数据**: 我们让SFT模型（第一步得到的模型）针对成千上万个提示，生成多份不同的完整回答。
2. **人类标注**: 我们将这些成对或成组的回答发送给一个标注团队。这些标注员**在他们自己的时间，异步地**完成比较和排序任务。他们不需要实时等待模型生成，模型也不需要等待他们。这个过程可以大规模并行，雇佣成百上千的人同时进行。
3. **训练奖励模型**: 我们用收集到的这些带有“偏好排序”的数据，去训练一个独立的**奖励模型（RM）**。这个RM的目标就是学习并模仿人类的偏好。经过训练，它就成了一个可以**瞬间**对任何回答给出“偏好分数”的AI。

**这个阶段的特点**：

- **人类参与**: 这是人类智慧注入的唯一阶段。
- **异步且可扩展**: 标注工作可以像工厂流水线一样分发出去，与模型训练完全解耦。
- **产出**: 一个能代替人类进行快速打分的“**AI裁判**”。

---

**阶段2：【在线】强化学习高速循环 (无人类参与，速度极快)**

现在，我们有了“AI裁判”（奖励模型），我们就可以进行真正的、高速的强化学习训练了。

1. **模型生成**: PPO模型（Agent）针对一个提示，生成一个完整的回答（Action）。
2. **AI裁判打分**: 这个回答被**立即**发送给**奖励模型（RM）**。
3. **获得奖励**: 奖励模型**瞬间**输出一个分数（Reward）。
4. **模型更新**: PPO算法利用这个奖励分数，更新PPO模型的参数。

**这个阶段的特点**：

- **完全自动化**: 整个循环中**没有任何人类**，只有模型和模型之间的交互。
- **速度极快**: 这个“生成-打分-更新”的循环可以在GPU上以极高的速度运行，每秒可以进行许多次。
- **产出**: 一个经过充分优化、行为与人类偏好对齐的最终语言模型。

---



### 2.2 Feedback 基本属性

#### 1. 评估性 vs. 矫正性 (Evaluative vs. Corrective)

- **评估性 (Evaluative)**
  - **含义**: 只提供一个关于“好坏”的**评价或分数**，但不告诉你“应该怎么改”。
  - **例子**:
    - 你写了一篇文章，我只给出一个评分：“**85分**”或者“**还不错**”。
    - 在RLHF中，奖励模型输出的那个单一奖励值（reward score）就是典型的评估性反馈。
    - “A比B好”这种偏好判断也属于评估性，因为它只评价了相对好坏。
  - **特点**: 信息量少，但提供起来非常简单。
- **矫正性 (Corrective)**
  - **含义**: 不仅告诉你哪里不好，还**直接给出了修改建议或正确答案**。
  - **例子**:
    - 你写了一篇文章，我直接在上面批改：“**这个词用得不对，应该换成‘璀璨’；这一段逻辑不通，应该调整为...**”
    - 在SFT（监督微调）阶段，我们提供给模型的“标准答案”就是最典型的矫正性反馈。
    - 一些更高级的反馈形式，比如让用户直接编辑模型的错误回答，也属于矫正性。
  - **特点**: 信息量极大，学习效率高，但获取成本也最高。

------

#### 2. 离线 vs. 在线 (Offline vs. Online)

这个描述的是反馈收集和模型训练的时间关系。

- **离线 (Offline)**
  - **含义**: **先一次性收集好所有反馈数据，然后再用这些固定的数据去训练模型**。训练开始后，就不再有新的反馈进来了。
  - **例子**:
    - 经典的RLHF流程就是离线的。我们先雇人花几个月时间标注好一个庞大的偏好数据集，然后用它来训练奖励模型和PPO模型。
    - 这就像出版一本教科书，一旦印刷完成，内容就固定了。
  - **特点**: 工程上简单，易于管理和复现。
- **在线 (Online)**
  - **含义**: **模型一边与环境（或用户）互动，一边实时地接收反馈，并持续地更新自己**。
  - **例子**:
    - 未来的智能助手可能具备在线学习能力。当你对它的回答点“踩”👍或“顶”👎时，它会立即微调自己的参数，争取下一次做得更好。
    - 这就像一个学徒跟着师傅干活，师傅随时指点，学徒随时改进。
  - **特点**: 适应性强，能实现个性化，但工程实现和算法稳定性要求极高。

------

#### 3. 策略相关 vs. 策略无关 (On-policy vs. Off-policy)

这个是强化学习里的一个核心技术概念，描述的是“产生数据的策略”和“正在学习的策略”是否是同一个。

- **策略相关 (On-policy)**
  - **含义**: **我必须用我“当前”的策略去探索，拿到数据后马上学习，然后就丢掉这些数据**。新一轮探索必须用“更新后”的策略。
  - **例子**: PPO算法就是典型的On-policy。它用当前的PPO模型生成一批回答，用奖励模型打分，学习一次，然后这批回答就不能再用了。它必须用更新后的PPO模型去生成新的回答才能继续学习。
  - **类比**: 你想学爬山，你必须自己亲自去爬（用当前的你），根据这次的经验（比如差点滑倒）来调整你的下一步计划。你不能用你朋友上周的爬山经验来指导你现在的每一步。
- **策略无关 (Off-policy)**
  - **含义**: **我“当前”学习的策略，可以使用“过去”的或其他策略产生的数据**。这使得数据可以被反复利用。
  - **例子**: DQN算法通过“经验回放池”实现了Off-policy。无论智能体在探索时用了多么愚蠢的旧策略，那些经验 `(s, a, r, s')` 都会被存起来，供当前最新的策略反复学习。
  - **类比**: 你可以从任何人的经验中学习，不管是你朋友的、你自己的，甚至是历史名人的失败教训（旧策略产生的数据），都可以用来优化你当前的行为。

------

#### 4. 完整轨迹 vs. 部分轨迹 (Full vs. Partial Trajectory)

这个描述的是反馈是针对一个完整的“故事”，还是只针对其中的一个“片段”。

- **完整轨迹 (Full Trajectory)**
  - **含义**: 反馈是针对智能体从开始到结束的**一整套完整行为**给出的。
  - **例子**:
    - 在RLHF中，对语言模型的整个回答（从第一个词到最后一个词）进行排序或打分，就是对完整轨迹的反馈。
    - 一盘棋下完后，我们才知道是赢是输，这个“输赢”就是对完整轨迹的反馈。
  - **特点**: 容易给，但“功劳/错误分配”困难（Credit Assignment Problem）。赢了一盘棋，到底是哪一步走得好？
- **部分轨迹 (Partial Trajectory)**
  - **含义**: 反馈是针对智能体在某个**中间状态**的行为给出的。
  - **例子**:
    - 在教机器人走路时，我们可以在它即将摔倒的那个瞬间就给一个负奖励，而不用等它完全摔倒。
    - 如果模型的回答写到一半就开始胡说八道，我们可以只针对“胡说八道”这部分给出反馈，而不是针对整个回答。
  - **特点**: 奖励信号更即时、更精确，但获取成本可能更高。

------



### 2.3 Feedback的基本类型以及实现方式

#### 1. 批评 (Critique)

- **含义**
  这是一种最简单的“否定”信号。用户指出某个模型的行为是**不好的**，但通常不会具体说明应该如何修正。
- **实现方式**
  在实践中，一个独立的“差评”信号（例如一个很大的负分）可能会导致强化学习训练不稳定。因此，通常会将“批评”转化为“对比”的一部分来处理。具体做法是：当用户对回答A给出差评时，系统会将A标记为**被拒绝的回答 (rejected response)**，同时将一个新生成的、或默认的空回答B作为**被选择的回答 (chosen response)**。这样就构成了一个 `(chosen=B, rejected=A)` 的数据对，可以被纳入标准的DPO或RLHF训练流程中。

#### 2. 对比 (Comparison)

- **含义**
  这是RLHF和DPO方法论的**核心**。用户在两个或多个选项中，选出**更好的那一个**。
- **实现方式**
  主要有两种主流方法：
  1. **RLHF**: 收集大量的偏好对 `(chosen, rejected)`，然后用这些数据来训练一个**奖励模型 (Reward Model)**。这个模型的目标是给“chosen”的回答打出的分数要远高于给“rejected”的回答打出的分数。
  2. **DPO**: 直接利用这些偏好对，通过一个专门设计的损失函数来直接优化策略模型。目标是让模型**提高生成“chosen”回答的概率**，同时**降低生成“rejected”回答的概率**。

#### 3. 跨时间反馈 (Feedback Across Time)

- **含义**
  这类反馈的目标不是单一的回答，而是针对**整个对话的流程**或模型**长期的行为**。
- **实现方式**
  这是一个实现起来很有挑战、尚在积极研究的领域。主要思路有：
  1. **对话级奖励**: 在一次完整的对话结束后，让用户给出一个**总体满意度评分**（例如1-5星）。这个分数被用作对整个对话“轨迹”的奖励。但其主要挑战在于**信用分配**，即难以判断对话的成功或失败究竟是哪一句话的功劳或责任。
  2. **扩展状态表示**: 将对话的**历史记录**也作为模型当前状态的一部分。这样，模型在做决策时就能“看到”之前的上下文，从而学会保持长期连贯性。

#### 4. 代理奖励 (Proxy Reward)

- **含义**
  人类不直接、实时地提供奖励，而是由一个**“代理”或“替身”**来提供。
- **实现方式**
  这正是RLHF流程用来解决效率和成本问题的关键工程方案——**奖励模型 (Reward Model)**。
  1. **离线训练代理**: 首先，离线收集大量人类给出的“对比”反馈数据。
  2. **学习人类偏好**: 用这些数据训练一个独立的模型，这个模型就是“代理”。它的任务是学习并模仿人类的品味，从而能够对任何回答进行快速打分。
  3. **在线高速训练**: 在后续的强化学习阶段，PPO模型生成的每一个回答都由这个**代理奖励模型**来打分，而不再需要人类的参与，从而让整个训练循环可以完全自动化并高速进行。

#### 5. 改进 (Improvement)

- **含义**
  这是一种信息量极大的**矫正性 (Corrective)** 反馈。用户不只是批评，而是**直接动手修改**模型的输出，提供一个他们认为**更好的版本**。
- **实现方式**
  1. **作为SFT数据**: 如果模型的回答是A，用户将其修改为A'，那么 `(prompt, A')` 就构成了一个高质量的**监督微调 (SFT)** 数据点。大量此类数据可以直接教会模型正确的回答方式。
  2. **作为偏好对**: 也可以将用户的改进版A'视为**绝对的赢家**，原始版A视为**绝对的输家**。这个“完美的”偏好对 `(chosen=A', rejected=A)` 可以被送入DPO或RLHF流程，通常能取得很好的效果。

#### 6. 自然语言 (Natural Language)

- **含义**
  这是最自然但也最难处理的一种反馈。用户直接用**日常语言**来告诉模型它哪里做得不好，或者下一步应该怎么做。
- **实现方式**
  这属于技术前沿领域，主要探索方向有：
  1. **作为新的指令**: 对于能力足够强大的模型，可以直接将用户的反馈（例如：“你的回答太专业了，请用小孩子的口吻重说一遍”）作为新一轮对话的指令来生成回答。
  2. **反馈解释模型**: 训练一个专门的“解释模型”。它的任务是读取原始对话和用户的自然语言反馈（例如“太啰嗦了”），然后**输出结构化的批评**（例如`{critique: 'length', direction: 'shorter'}`）。这个结构化信号再被用于后续的训练。







## 3 如何收集偏好Label?











